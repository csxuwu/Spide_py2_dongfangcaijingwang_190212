

--爬虫一共四个步骤
=====================================================================================================================================================================
	-1 明确目标，即明确在哪个范围或者网站去搜索
	-2 爬，将网站所有的内容全爬下来
	-3 取，去掉对我们没用处的数据
	-4 处理数据，按照我们想要的方式存储和使用
=====================================================================================================================================================================

--爬：
=====================================================================================================================================================================
	要点：
	-----------------------------------------------------------------------------------------------------------------------------------------------------------------
		-1 url：目的网址
		-2 Request：请求报文对象，可以包含url、headers和data等等信息，而不仅仅发送url，可以对url进行伪装，也可以通过data，将数据post到服务器上。简单说，就是一个伪装过的请求报文
			url：目标网址
			header：请求报文的头部
				user-agent：浏览器名称，伪装爬虫的第一步
				connection：链接类型
				accept：传输文件的类型
				accept-encoding：文件编解码格式
				cookie：浏览器中的小型数据体，记载和服务器相关的用户信息，通过客户端记录的信息确定用户信息（Session：通过在服务器端记录的信息确定用户信息）
				...
			data
			...
		-3 urlopen
			向服务器发送请求报文，返回服务器的响应报文
			定制open：
				1) 后面通过【Handler处理器】对这个方法也会进一步的丰富
				2) HTTPPasswordMgrWithDefaultRealm  ProxyBasicAuthHandler
	-----------------------------------------------------------------------------------------------------------------------------------------------------------------

	Handler处理器 和 自定义Opener
	-----------------------------------------------------------------------------------------------------------------------------------------------------------------
		opener：urllib2.OpenerDirector的实例，之前使用到的urlopen，是一个特殊的opener（模块搭建好的）
		但是urlopen() 不支持代理、cookie等其他的HTTP/HTTPS高级功能，如果要支持：
			-1 使用相关的Handler处理器，创建特定功能的处理器对象
			-2 通过urllib2.build_opener() 使用这些处理器对象，创建自定义opener对象
			-3 使用自定义的opener对象，调用open()方法发送请求
		如果程序里所有的请求都使用自顶以的opener，可以使用urllib2.install_opener对象定义为全局opener，表示凡是调用urlopen，都将使用这个opener
	-----------------------------------------------------------------------------------------------------------------------------------------------------------------

	ProxyHandler处理器（IP代理设置）
	-----------------------------------------------------------------------------------------------------------------------------------------------------------------
	    使用代理IP，爬虫/反爬虫的第二大招
	    当网站监测到某一时间段内，一个IP的访问次数过多，不像一个正常人的节奏，就会禁止这个IP的访问
	    通过ProxyHandler设置代理服务器，下面是使用自定义的opener来使用代理
	-----------------------------------------------------------------------------------------------------------------------------------------------------------------
	
	HTTPPasswordMgrWithDefaultRealm()
	-----------------------------------------------------------------------------------------------------------------------------------------------------------------
		伪装爬虫：使用用户、密码、代理IP登录目标网址
			HTTPPasswordMgrWithDefaultRealm：管理用户、密码、代理IP
			ProxyBasicAuthHandler：代理基础用户

	    HTTPPasswordMgrWithDefaultRealm()类将创建一个密码管理对象，用来存储HTTP请求相关的用户和密码，主要有两个应用场景：
	        -1 验证代理授权的用户和密码（ProxyBasicAuthHandler()）
	        -2 验证web客户端的用户名和密码（HTTPBasicAuthHandler()）
	    
	    ProxyBasicAuthHandler(代理授权验证)
	        如果我们使用之前的代码来使用私密代理，会报 HTTP 407 错误，表示代理没有通过身份验证：
	         urllib2.HTTPError: HTTP Error 407: Proxy Authentication Required 
	            改写代码，通过：
	                HTTPPasswordMgrWithDefaultRealm() ：来保存私密代理的用户密码
	                ProxyBasicAuthHandler() ：来处理代理的身份验证。
	----------------------------------------------------------------------------------------------------------------------------------------------------------------- 
	
	Cookie
	-----------------------------------------------------------------------------------------------------------------------------------------------------------------
		-1 Cookie ：某些网址服务器为了辨别用户的身份和进行session跟踪，而储存在用户浏览器上的文本文件，Cookies可以保持登录信息到用户下次与服务器的会话
		-2 Cookie原理：HTTP是无状态的面向连接的协议，为了保持连接状态，引入了Cookie机制，Cookie是http消息头的一种属性，包括：
			Cookie名字（Name）
			Cookie的值（Value）
			Cookie的过期时间（Expires/Max-Age）
			Cookie作用路径（Path）
			Cookie所在域名（Domain），
			使用Cookie进行安全连接（Secure）。
			前两个参数是Cookie应用的必要条件，另外，还包括Cookie大小（Size，不同浏览器对Cookie个数及大小限制是有差异的）。

			Cookie由变量名和值组成，根据Netscape公司的规定，Cookie格式如：
				 Set－Cookie: NAME=VALUE；Expires=DATE；Path=PATH；Domain=DOMAIN_NAME；SECURE 
		-3 Cookie的应用：
			在爬虫中最典型的应用是判定注册用户是否已经登录网站，用户可能会得到提示，是否在下一次进入此网站时保留用户信息以便简化登录手续
		-4 利用cookie信息，模拟登录
			1) 先进行一次请求，将网站的cookie存储，HTTP GET
			2) 利用刚刚获得的cookie的信息再次请求，模拟登录，HTTP POST
			但是这种方法的模拟登录存在几个问题:
				1) passwd有些是明文发送，有些加密，甚至是动态加密，同时包括其他数据的加密信息，这就难以获取了，需要通过查看js源码获取加密算吗，然后破解加密。
				2) 不能保证每个网站都行
		-5 cookielib库 和 HTTPCookieProcessor处理器
			cookielib：提供用于存储cookie的对象
			HTTPCookieProcessor：处理这些cookie对象，并构建handler对象
			1) cookielib
				主要有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar
					CookieJar：管理HTTP cookie值、存储HTTP请求生成的cookie、向传出的HTTP请求添加cookie的对象。整个cookie都存储在内存中，对CoookieJar实例进行垃圾回收后cookie也将丢失。
					FileCookieJar(filename,delayload=None,policy=None)：从CookieJar派生而来，用来创建FileCookieJar实例，检索cookie信息并将cookie存储到文件中。filename是存储cookie的文件名。delayload为True时支持延迟访问访问文件，即只有在需要时才读取文件或在文件中存储数据。
					MozillaCookieJar(filename,delayload=None,policy=None)：从FileCookieJar派生而来，创建与 Mozilla浏览器 cookies.txt兼容 的FileCookieJar实例。
					LWPCookieJar(filename,delayload=None,policy=None)：从FileCookieJar派生而来，创建与 libwww-perl标准的 Set-Cookie3 文件格式 兼容的FileCookieJar实例。
				大多数只用CookieJar()，如果需要和本地文件交互，就用MozillaCookieJar()、LWPCookieJar()
			2)

	-----------------------------------------------------------------------------------------------------------------------------------------------------------------        
=====================================================================================================================================================================

--取
=====================================================================================================================================================================
	页面解析和数据提取
	-----------------------------------------------------------------------------------------------------------------------------------------------------------------
	网站内容通常分为:
		非结构化的数据
		结构化的数据
	-1 非结构化的数据	先有数据，再有结构
		1) 文本、电话号码、邮箱地址
			正则表达式
		2) HTML文件
			正则表达式、XPath、CSS选择器
	-2 结构化的数据		先有结构，再有数据
		1) JSON文件
			JSON Path
			转化成Python类型进行操作（json类）
		2) XML文件
			转化成Python类型（xmltodict）
			XPath
			CSS选择器
			正则表达式
	-3 不同类型的数据	采取不同的方式处理

	-----------------------------------------------------------------------------------------------------------------------------------------------------------------

	正则表达式
	-----------------------------------------------------------------------------------------------------------------------------------------------------------------
	-1 常用的表达式可以查表，或者百度
	-2 re模块几个常用的方法：
		1) compile：生产pattern对象
		2) pattern 对象的方法：
			match		从起始位置开始查找，一次匹配
			search      从任何位置开始查找，一次匹配
			findall		全部匹配，返回列表
			finditer	全部匹配，返回迭代器
			split       分割字符串，返回列表
			sub         替换
		3) 匹配中文
			[u4e00-u9fa5]
		4) 贪婪模式与非贪婪模式
			a) 贪婪模式：	在整个表达式匹配成功的前提下，尽可能多的匹配（*）
			b) 非贪婪模式：	在整个表达式匹配成功的前提下，尽可能少的匹配（？）
			python里数量词默认是贪婪的
		5) 正则表达式在线测试工具：
			http://tool.oschina.net/regex/
	-----------------------------------------------------------------------------------------------------------------------------------------------------------------

	XML
	-----------------------------------------------------------------------------------------------------------------------------------------------------------------
	XML官方文档		http://www.w3school.com.cn/xml/index.asp
	XPath官方文档	http://www.w3school.com.cn/xpath/index.asp
	lxml官方文档	http://lxml.de/index.html

	-1 XML
		指可扩展标记语言(EXtensible Markup Language)
		是一种标记语言，类似HTML
		设计宗旨：传输数据，不是显示数据
		标签需要自定义
		具有自我描述性
		是W3C的推荐标准
	-2 XML与HTML的区别
		数据格式					描述						设计目标
		XML 			EXtensible Markup Language		传输和存储数据，其焦点是显示数据的内容
						（可扩展标记语言）
		HTML 			HyperText Markup Language		显示数据，以及如何更好的显示数据
						（超文本标记）
		HTML DOM 		Document Object Model for 		通过HTML DOM可以访问所有的HTML元素，连同他们所包含的文本和属性
						HTML（文本对象模型）			可以对其中的内容进行修改和删除，同时可以创建新的元素。

		XML文档示例：
			<?xml version="1.0" encoding="utf-8"?>

			<bookstore> 
			  <book category="cooking"> 
			    <title lang="en">Everyday Italian</title>  
			    <author>Giada De Laurentiis</author>  
			    <year>2005</year>  
			    <price>30.00</price> 
			  </book>
			</bookstore> 

		HTML DOM 模型示例：
			HTML DOM定义了访问和操作HTML文档的标准方法，以树结构方式表达HTML文档
	-3 XML的节点关系
		1) 父(Parent)
			紧邻的上一级标签
		2) 子(Children)
			紧邻的下一级标签
		3) 同胞(Sibling)
			同级的标签，也可以叫“兄弟”
		4) 先辈(Ancestor)
			所有上级标签，可以说是“父辈”的扩展
		5) 后代(Descendant)
			所有下级标签，可以说是“子辈”的扩展
	-4 XPath(XML Path Language)
		在XML文档中查找信息的语言，可以用在XML文档中对元素和属性进行遍历
		1) XPath开发工具
			a) 开源的XPath表达式编辑工具：XML Quire（XML格式文件可用）
			b) Chrome插件XPath Helper
			c) Firefox插件 XPaht Checker
		2) 选取节点 		标签的应用可查表
		3) 谓语
		4) 选取未知节点
		5) 选取若干路径
		6) XPath运算符
		7) lxml库
	----------------------------------------------------------------------------------------------------------------------------------------------------------------

	Beautifulsoup4
	----------------------------------------------------------------------------------------------------------------------------------------------------------------
	beautifulsoup4官方文档：http://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/

	-1 beautifulsoup：和lxml一样，是一个HTML/XML解析器，主要功能是如何解析和提取HTML/XML数据
		1) 与lxml不同的是：lxml只是局部遍历，而beautifulsoup是基于HTML DOM，所以性能比beautifulsoup低
		2) beautifulsoup用来解析html比较简单，支持CSS选择器、python标准库中HTML解析器，也支持lxml的XML解析器
	-2 四大对象种类
		Tag
		NavigableString
		BeautifulSoup
		Comment

		1) Tag
			是HTML中的一个标签，例如：
				<head><title>The Dormouse's story</title></head>
				<a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>
				<p class="title" name="dromouse"><b>The Dormouse's story</b></p>
			将html文档转为beautifulsoup对象soup
			直接通过soup.标签，即可获得该整个标签的内容。这些对象的类型是bs4.element.Tag。这种方法获取的是html所有内容中第一个符合要求的标签

			两个属性：
				name、attrs
				name 	标签的名称
				attrs	标签所有的属性，返回的是一个字典
		2) NavigableString
			即标签的内容
			soup.title.string 获得标签的内容，其类型为bs4.element.NavigableString
		3) BeautifulSoup
			该对象表示的是一个文档的内容，大部分时候可以将它看做是Tag对象
		4) Comment
			是一个特殊的NavigableString对象，其输出内容不包括注释符号
	-3 遍历文档树
		1) 直接子节点
			.content 	将tag的子节点以列表的方式输出，利用索引读取元素
			.children 	将tag的子节点存储到一个list生成器对象，通过遍历获取元素
		2) 所有子节点
			.descendants 对所有tag的子孙节点进行递归循环，和children类似，通过遍历获取其中的内容
		3) 节点内容
			.string 	获得节点的内容。如果一个标签没有内嵌的子标签，那么返回的是标签里的内容；如果标签里内嵌了仅1个子标签，那么返回子标签的内容
	-4 搜索文档树
		1) find_all(name, attrs, recursive, text, **kwargs)
			name:查找所有名字为name的tag，字符串对象会被自动忽略
				a) 传字符串
				b) 传正则表达式
				c) 传列表
			text：可搜索文档中的字符串内容，与name参数类似，可接受 字符串、正则表达式、列表
		2) CSS选择器
			写CSS时，标签名不需要任何的修饰，类名前加"."，id名前加"#"
				a) 标签名查找	soup.select('title')
				b) 类名查找		soup.select('.sister')
				c) id名查找		soup.select('#link1')
				d) 组合查找
					组合查找即和写 class 文件时，标签名与类名、id名进行的组合原理是一样的，例如查找 p 标签中，id 等于 link1的内容，二者需要用空格分开	
					print soup.select('p #link1')	
					print soup.select("head > title")	直接子标签查找，则使用 > 分隔
				e) 属性查找
					查找时可以加入属性元素，属性需要用中括号括起来，注意【属性】和【标签】属于【同一节点】，所以中间不能加空格，否则无法匹配到
					print soup.select('a[class="sister"]')
					#[<a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>, <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>, <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]
				f) 以上的select方法返回结果，都是列表形式，可以遍历形式输出，用get_text()方法获取它的内容
					soup = BeautifulSoup(html, 'lxml')
					print soup.select('title')[0].get_text()
					for title in soup.select('title'):
					    print title.get_text()

	----------------------------------------------------------------------------------------------------------------------------------------------------------------

	JSON与JsonPATH
	----------------------------------------------------------------------------------------------------------------------------------------------------------------
	官方文档：http://docs.python.org/library/json.html
	在线解析网站：http://www.json.cn/#
		-1 JSON(JavaScript Object Notation)：轻量级数据交换格式，易于阅读和编写，同时方便机器解析和生产。适用于进行数据交互的场景。比如网站前台和后台之间的数据交互
		-2 JSON
			简单说，是javascript中的对象和数组，所以这两种结构的就是对象和数组两种结构
			1) 对象
				对象在js中表示为{}括起来的内容，数据结构为{key:value,key:value,...}的键值对结构（类似字典）
				key：对象
				value：

			2) 数组
				数组在js中是[]括起来的，数据结构为["a","b",...]
				取值方式和所有语言一样，使用索引获取，字段值的类型可以是数字、字符串、数组和对象几种
		-3 import json
			json提供了四个功能：dumps、dump、loads、load
			1) json.loads()
				将json格式字符串解码转为python对象
			2) json.load()	
				读取文件中json形式的字符串元素，转化成python类型
			3) json.dumps()
				将python类型转为字符串，返回一个str对象，把一个python对象编码转换成json字符串
			4) json.dump()
				将python内置类型序列化为json对象后写入文件
		-4 JsonPath 
			下载地址：https://pypi.python.org/pypi/jsonpath
			安装方法：点击 Download URL 链接下载jsonpath，解压之后执行 python setup.py install 
			官方文档：http://goessner.net/articles/JsonPath

			JsonPath是一种信息抽取类库，是从JSON文档中抽取指定信息的工具，提供多种语言的实现版本。包括：Javascript\Python\PHP
			JsonPath对于JSON，相当于XPath对于XML
			
	----------------------------------------------------------------------------------------------------------------------------------------------------------------

	正则表达式、xml(xpath)、beautifulsoup
	----------------------------------------------------------------------------------------------------------------------------------------------------------------
		获取网页的内容，只是第一步，从网页内容中提取数据才是关键，数据包括：文本、图片链接、视频链接等等。
		将整个爬取内容看成一个很大的字符串
		-1 正则表达式：
			通过正则匹配，直接从整个字符串中匹配，不需要遍历，效率最高，但是编写正则表达式难度较大
		-2 xml(path)：
			将爬取的html内容转为html文档，在这个文档上进行一些的操作，是这个文档上的局部遍历
			将html文档内容看成是一个树形的结构，节点之间的关系和树的类似，通过节点操作，来获取相应节点下的内容：文本，链接等等
		-3 beautifulsoup：
			和xml类似的思想，不同的是对整个文档进行全局的遍历，效率相对低

	----------------------------------------------------------------------------------------------------------------------------------------------------------------
=====================================================================================================================================================================